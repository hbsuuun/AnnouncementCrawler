import os
import time
import random
import requests
import re
import argparse
import json
from pathlib import Path
from datetime import datetime, timezone
from tqdm import tqdm

BASE_URL = "https://www.cninfo.com.cn/new/hisAnnouncement/query"
PDF_BASE = "https://static.cninfo.com.cn/"
ORGID_MAP_FILE = Path("stockcodes/stock_orgids.json")
with open(ORGID_MAP_FILE, "r", encoding="utf-8") as f:
    STOCK_ORGIDS = json.load(f)
# äº¤æ˜“æ‰€æ˜ å°„
EXCHANGE_MAP = {
    'sz': 'SZ',    # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
    'sh': 'SH',    # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
    'szse': 'SZ',  # æ·±åœ³è¯åˆ¸äº¤æ˜“æ‰€
    'sse': 'SH',   # ä¸Šæµ·è¯åˆ¸äº¤æ˜“æ‰€
}

def get_random_timeout(min_timeout=8, max_timeout=12):
    """è¿”å›æŒ‡å®šèŒƒå›´å†…çš„éšæœºè¶…æ—¶æ—¶é—´"""
    return random.uniform(min_timeout, max_timeout)

def load_downloaded_ids(save_dir):
    """ä» save_dir å†…åŠ è½½å·²ä¸‹è½½çš„ announcementId é›†åˆ"""
    ids_file = os.path.join(save_dir, ".downloaded_ids.json")
    if not os.path.exists(ids_file):
        return set()
    
    try:
        with open(ids_file, "r", encoding="utf-8") as f:
            data = json.load(f)
            return set(data.get("downloaded_ids", []))
    except Exception as e:
        print(f"âš ï¸ åŠ è½½å·²ä¸‹è½½IDåˆ—è¡¨å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹")
        return set()

def save_downloaded_ids(save_dir, downloaded_ids):
    """ä¿å­˜å·²ä¸‹è½½çš„ announcementId é›†åˆåˆ° save_dir å†…"""
    ids_file = os.path.join(save_dir, ".downloaded_ids.json")
    data = {
        "downloaded_ids": list(downloaded_ids),
        "last_update": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S"),
        "total_count": len(downloaded_ids)
    }
    try:
        with open(ids_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜å·²ä¸‹è½½IDåˆ—è¡¨å¤±è´¥: {e}")

def cninfo_stock_param(code):
    """æŠŠ 000001.SZ / 600000.SH è½¬æˆå·¨æ½®éœ€è¦çš„ sz000001 / sh600000"""
    if not code or '.' not in code:
        return code
    digits, suffix = code.split('.')
    return f"{suffix.lower()}{digits}"

def fetch_announcements(stock_codes=None, page_num=1, page_size=30, timeout_min=8, timeout_max=12, max_retries=3, retry_delay=2, days=None):
    """
    è·å–å…¬å‘Šåˆ—è¡¨
    :param stock_codes: è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆæ ¼å¼ï¼š["000001.SZ", "600000.SH"]ï¼‰
    :param page_num: é¡µç 
    :param page_size: æ¯é¡µæ•°é‡
    :param timeout_min: æœ€å°è¶…æ—¶æ—¶é—´
    :param timeout_max: æœ€å¤§è¶…æ—¶æ—¶é—´
    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :param retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰
    :return: å…¬å‘Šåˆ—è¡¨
    """
    # è®¡ç®—æ—¥æœŸèŒƒå›´
    se_date = ""
    if days is not None and days > 0:
        end_date = datetime.now(timezone.utc).strftime('%Y-%m-%d')
        start_ts = time.time() - (days * 24 * 60 * 60)
        start_date = datetime.fromtimestamp(start_ts, tz=timezone.utc).strftime('%Y-%m-%d')
        se_date = f"{start_date}~{end_date}"
        print(f"   ğŸ“… æ—¥æœŸèŒƒå›´: {start_date} ~ {end_date} (è¿‘ {days} å¤©)")
    
    params = {
        "stock": "",
        "pageNum": page_num,
        "pageSize": page_size,
        "tabName": "fulltext",
        "plate": "",
        "seDate": se_date,  # åŠ¨æ€è®¾ç½®æ—¥æœŸèŒƒå›´
        "column": "szse",
        "category": "",
        "searchkey": "",
        "secid": "",
        "sortName": "",
        "sortType": "",
        "isHLtitle": "true",
    }

    if stock_codes and isinstance(stock_codes, list) and len(stock_codes) > 0:
        stock_pairs = []
        for code in stock_codes:
            digits, suffix = code.split(".")
            org_id = STOCK_ORGIDS.get(digits)
            if not org_id:
                print(f"âš ï¸ æœªæ‰¾åˆ° orgId: {code}ï¼Œè·³è¿‡è¯¥è‚¡ç¥¨")
                continue
            stock_pairs.append(f"{digits},{org_id}")
        if stock_pairs:
            params["stock"] = ";".join(stock_pairs)

            first_code = stock_codes[0]
            params["column"] = "sse" if first_code.endswith(".SH") else "szse"

    # é‡è¯•é€»è¾‘
    for attempt in range(max_retries + 1):
        try:
            timeout = get_random_timeout(timeout_min, timeout_max)
            resp = requests.post(BASE_URL, data=params, timeout=timeout)
            resp.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
            data = resp.json()
            return data.get("announcements", [])
        except requests.exceptions.Timeout as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)  # æŒ‡æ•°é€€é¿
                print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼ˆç¬¬ {page_num} é¡µï¼Œå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: timeout={timeout:.2f}ç§’ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼ˆç¬¬ {page_num} é¡µï¼‰: å·²é‡è¯• {max_retries} æ¬¡ä»å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)
                print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼ˆç¬¬ {page_num} é¡µï¼Œå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: {str(e)}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼ˆç¬¬ {page_num} é¡µï¼‰: å·²é‡è¯• {max_retries} æ¬¡ä»å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
        except Exception as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)
                print(f"âš ï¸ æœªçŸ¥é”™è¯¯ï¼ˆç¬¬ {page_num} é¡µï¼Œå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: {str(e)}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                print(f"âš ï¸ æœªçŸ¥é”™è¯¯ï¼ˆç¬¬ {page_num} é¡µï¼‰: å·²é‡è¯• {max_retries} æ¬¡ä»å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨")
                return []
    
    return []

def sanitize_filename(filename):
    """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
    illegal_chars = r'[<>:"/\\|?*]'
    filename = re.sub(illegal_chars, '_', filename)
    if len(filename) > 200:
        filename = filename[:200]
    return filename

def normalize_stock_code(stock_code):
    """å°†è¾“å…¥ç»Ÿä¸€è½¬æ¢æˆ 6 ä½æ•°å­— + .SZ/.SH çš„æ ‡å‡†æ ¼å¼"""
    stock_code = str(stock_code).strip().upper()

    # å·²ç»æ˜¯æ ‡å‡†æ ¼å¼ï¼š000001.SZ / 600000.SH
    if re.match(r'^\d{6}\.(SZ|SH)$', stock_code):
        return stock_code

    # çº¯ 6 ä½æ•°å­—ï¼šæŒ‰é¦–ä½åˆ¤æ–­äº¤æ˜“æ‰€
    if re.match(r'^\d{6}$', stock_code):
        if stock_code.startswith('6'):
            return f"{stock_code}.SH"
        elif stock_code.startswith(('0', '3')):
            return f"{stock_code}.SZ"
        else:
            return f"{stock_code}.SZ"

    # å…¶ä»–å­—ç¬¦ä¸²ï¼šæå– 6 ä½æ•°å­—åé€’å½’å¤„ç†
    digits = re.findall(r'\d{6}', stock_code)
    if digits:
        return normalize_stock_code(digits[0])

    raise ValueError(
        f"æ— æ•ˆçš„è‚¡ç¥¨ä»£ç æ ¼å¼ï¼š{stock_code}ï¼ˆæ”¯æŒæ ¼å¼ï¼š600000ã€000001.SZã€600000.SHï¼‰"
    )


def get_announcement_date(item):
    """æŠŠ announcementTime ç»Ÿä¸€è½¬æ¢ä¸º YYYY-MM-DDï¼ˆå…¬å‘Šå‘å¸ƒæ—¶é—´ï¼‰ï¼Œç¼ºçœåˆ™è¿”å›ç©ºå­—ç¬¦ä¸²"""
    raw_time = item.get('announcementTime')
    if raw_time in (None, ''):
        return ''

    # å¦‚æœæ˜¯æ•°å­—æˆ–çº¯æ•°å­—å­—ç¬¦ä¸² â†’ å½“æˆ Unix æ—¶é—´æˆ³
    if isinstance(raw_time, (int, float)) or (isinstance(raw_time, str) and raw_time.isdigit()):
        ts = int(raw_time)
        if ts > 1_000_000_000_000:  # æ¯«ç§’æ—¶é—´æˆ³
            ts //= 1000
        return datetime.fromtimestamp(ts, tz=timezone.utc).strftime('%Y-%m-%d')

    # å¦åˆ™è§†ä¸ºå·²æœ‰çš„æ—¥æœŸå­—ç¬¦ä¸²ï¼Œå–å‰ 10 ä½ï¼ˆå¦‚ '2025-11-14'ï¼‰
    return str(raw_time)[:10]

def generate_download_report(save_dir, stock_codes, all_items, stock_groups, 
                             success_pdf, success_html, pdf_items, html_items,
                             requested_codes, missing_codes, downloaded_ids_before, downloaded_ids_after,
                             args):
    """åœ¨ save_dir å†…ç”Ÿæˆæœ¬æ¬¡ä¸‹è½½çš„è¯¦ç»†æŠ¥å‘Š"""
    report_file = os.path.join(save_dir, f"download_report_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.md")
    
    report_time = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S UTC")
    new_downloads = len(downloaded_ids_after) - downloaded_ids_before
    
    report_content = f"""# å…¬å‘Šä¸‹è½½æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **ä¸‹è½½æ—¶é—´**: {report_time}
- **ä¿å­˜ç›®å½•**: {save_dir}
- **æŒ‡å®šè‚¡ç¥¨æ•°é‡**: {len(requested_codes) if requested_codes else 'å…¨éƒ¨è‚¡ç¥¨'}
- **å®é™…è·å–å…¬å‘Šçš„è‚¡ç¥¨æ•°**: {len(stock_groups)}

## è¿è¡Œå‚æ•°
- **è‚¡ç¥¨ä»£ç æ–‡ä»¶**: {args.stock_file if args.stock_file else 'æœªæŒ‡å®š'}
- **è‚¡ç¥¨ä»£ç **: {args.stock_code if args.stock_code else 'æœªæŒ‡å®š'}
- **æœ€å¤§æŠ“å–æ€»æ•°**: {args.max_items_total}
- **æ¯é¡µæ•°é‡**: {args.page_size}
- **è¯·æ±‚è¶…æ—¶**: {args.timeout_min}-{args.timeout_max} ç§’
- **è¯·æ±‚å»¶è¿Ÿ**: {args.delay_min}-{args.delay_max} ç§’
- **ä¸‹è½½å»¶è¿Ÿ**: {args.download_delay_min}-{args.download_delay_max} ç§’
- **ä»…PDFæ¨¡å¼**: {'æ˜¯' if args.no_html else 'å¦'}

## ä¸‹è½½ç»Ÿè®¡

### æ€»ä½“æƒ…å†µ
- **è·å–å…¬å‘Šæ€»æ•°**: {len(all_items)} æ¡
- **PDFå…¬å‘Šæ•°**: {len(pdf_items)} æ¡
- **HTMLå…¬å‘Šæ•°**: {len(html_items)} æ¡
- **PDFä¸‹è½½æˆåŠŸ**: {success_pdf}/{len(pdf_items)} ä»½
- **HTMLä¸‹è½½æˆåŠŸ**: {success_html}/{len(html_items)} ä»½
- **æœ¬æ¬¡æ–°å¢ä¸‹è½½**: {new_downloads} æ¡ï¼ˆç´¯è®¡å·²ä¸‹è½½: {len(downloaded_ids_after)} æ¡ï¼‰

### å„è‚¡ç¥¨å…¬å‘Šæ•°é‡
"""
    
    for sec_code, items in sorted(stock_groups.items()):
        sec_pdf = len([i for i in items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")])
        sec_html = len(items) - sec_pdf
        report_content += f"- **{sec_code}**: å…± {len(items)} æ¡ï¼ˆPDF: {sec_pdf}, HTML: {sec_html}ï¼‰\n"
    
    report_content += f"""
## ä¸‹è½½è¯¦æƒ…

### å„è‚¡ç¥¨ä¸‹è½½ç»Ÿè®¡
"""
    
    for sec_code, items in sorted(stock_groups.items()):
        sec_pdf = len([i for i in items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")])
        sec_html = len(items) - sec_pdf
        sec_success_pdf = len([i for i in items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf") and os.path.exists(os.path.join(save_dir, sec_code, f"{sec_code}_{get_announcement_date(i)}_{sanitize_filename(i.get('announcementTitle', 'Unknown'))}.pdf"))])
        sec_success_html = 0
        if not args.no_html:
            sec_success_html = len([i for i in items if not ("adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")) and os.path.exists(os.path.join(save_dir, sec_code, f"{sec_code}_{get_announcement_date(i)}_{sanitize_filename(i.get('announcementTitle', 'Unknown'))}.html"))])
        report_content += f"- **{sec_code}**: PDF {sec_success_pdf}/{sec_pdf} ä»½, HTML {sec_success_html}/{sec_html} ä»½\n"
    
    # è‚¡ç¥¨ä»£ç åˆ—è¡¨ï¼ˆæ”¾åˆ°æœ€åï¼Œä½¿ç”¨è¡¨æ ¼æ ¼å¼ï¼‰
    report_content += f"""
---

## é™„å½•

### æœ¬æ¬¡è¯·æ±‚çš„è‚¡ç¥¨ä»£ç 
"""
    
    if stock_codes:
        # æŒ‰æ¯è¡Œ6ä¸ªä»£ç çš„è¡¨æ ¼æ ¼å¼æ˜¾ç¤º
        codes_per_row = 6
        report_content += "\n| " + " | ".join([f"åˆ—{i+1}" for i in range(codes_per_row)]) + " |\n"
        report_content += "|" + "|".join([" --- " for _ in range(codes_per_row)]) + "|\n"
        
        for i in range(0, len(stock_codes), codes_per_row):
            row_codes = stock_codes[i:i + codes_per_row]
            # è¡¥é½ç©ºå•å…ƒæ ¼
            while len(row_codes) < codes_per_row:
                row_codes.append("")
            report_content += "| " + " | ".join(row_codes) + " |\n"
    else:
        report_content += "\n- å…¨éƒ¨è‚¡ç¥¨ï¼ˆæœªæŒ‡å®šï¼‰\n"
    
    # æœªè·å–åˆ°å…¬å‘Šçš„è‚¡ç¥¨ï¼ˆæ”¾åˆ°æœ€åï¼‰
    if missing_codes:
        report_content += f"""
### âš ï¸ æœªè·å–åˆ°å…¬å‘Šçš„è‚¡ç¥¨

"""
        # åŒæ ·ä½¿ç”¨è¡¨æ ¼æ ¼å¼
        codes_per_row = 6
        report_content += "| " + " | ".join([f"åˆ—{i+1}" for i in range(codes_per_row)]) + " |\n"
        report_content += "|" + "|".join([" --- " for _ in range(codes_per_row)]) + "|\n"
        
        for i in range(0, len(missing_codes), codes_per_row):
            row_codes = missing_codes[i:i + codes_per_row]
            while len(row_codes) < codes_per_row:
                row_codes.append("")
            report_content += "| " + " | ".join(row_codes) + " |\n"
    else:
        report_content += "\n### âœ… æ‰€æœ‰æŒ‡å®šè‚¡ç¥¨å‡è·å–åˆ°è‡³å°‘ä¸€æ¡å…¬å‘Š\n"
    
    report_content += f"""
---
*æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}*
"""
    
    try:
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(report_content)
        print(f"ğŸ“„ ä¸‹è½½æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ä¸‹è½½æŠ¥å‘Šå¤±è´¥: {e}")

def download_pdf(item, save_dir, downloaded_ids, timeout_min=8, timeout_max=12, output_func=print, max_retries=3, retry_delay=1):
    """ä¸‹è½½PDFå…¬å‘Š"""
    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½
    announcement_id = item.get('announcementId')
    if announcement_id and announcement_id in downloaded_ids:
        output_func(f"â­ï¸  è·³è¿‡å·²ä¸‹è½½: {item.get('announcementTitle', 'Unknown')} (ID: {announcement_id})")
        return False
    
    # æ£€æŸ¥adjunctUrlæ˜¯å¦å­˜åœ¨ä¸”æ˜¯PDFæ ¼å¼ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
    if "adjunctUrl" not in item:
        output_func(f"âš ï¸ ç¼ºå°‘adjunctUrlå­—æ®µ: {item.get('announcementTitle', 'Unknown')}")
        return False
    
    if not item["adjunctUrl"].lower().endswith(".pdf"):
        output_func(f"âš ï¸ éPDFæ ¼å¼: {item['adjunctUrl']} | æ ‡é¢˜: {item.get('announcementTitle', 'Unknown')}")
        return False

    url = PDF_BASE + item["adjunctUrl"]
    sec_code = item.get('secCode', 'unknown')
    # åˆ›å»ºè‚¡ç¥¨ä»£ç å¯¹åº”çš„å­ç›®å½•
    stock_dir = os.path.join(save_dir, sec_code)
    if not os.path.exists(stock_dir):
        os.makedirs(stock_dir)
    
    # æ–‡ä»¶åï¼šè‚¡ç¥¨ä»£ç _å…¬å‘Šæ ‡é¢˜_æ—¥æœŸ.pdfï¼ˆé¿å…é‡å¤ï¼‰
    announcement_time = get_announcement_date(item)
    filename_parts = [sec_code]
    if announcement_time:
        filename_parts.append(announcement_time)
    filename_parts.append(sanitize_filename(item.get('announcementTitle', 'Unknown')))
    filename = "_".join(filename_parts) + ".pdf"
    filepath = os.path.join(stock_dir, filename)
    
    # é‡è¯•é€»è¾‘
    for attempt in range(max_retries + 1):
        try:
            timeout = get_random_timeout(timeout_min, timeout_max)
            
            pdf_resp = requests.get(url, timeout=timeout)
            
            if pdf_resp.status_code != 200:
                raise ValueError(f"HTTPçŠ¶æ€ç é”™è¯¯: {pdf_resp.status_code}")
            
            if len(pdf_resp.content) == 0:
                raise ValueError("å“åº”å†…å®¹ä¸ºç©º")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯PDFæ ¼å¼
            if not pdf_resp.content.startswith(b'%PDF'):
                raise ValueError("å“åº”å†…å®¹ä¸æ˜¯PDFæ ¼å¼")
            
            # ä¿å­˜æ–‡ä»¶
            with open(filepath, "wb") as f:
                f.write(pdf_resp.content)
            
            # éªŒè¯æ–‡ä»¶
            if not os.path.exists(filepath) or os.path.getsize(filepath) == 0:
                raise IOError("æ–‡ä»¶ä¿å­˜å¤±è´¥")
            
            file_size = os.path.getsize(filepath)
            output_func(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} (å¤§å°: {file_size} å­—èŠ‚)")
            
            # ä¸‹è½½æˆåŠŸåè®°å½•ID
            if announcement_id:
                downloaded_ids.add(announcement_id)
            
            return True
            
        except requests.exceptions.Timeout as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)
                output_func(f"âš ï¸ è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: {filename}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                output_func(f"âŒ è¯·æ±‚è¶…æ—¶: {filename} | timeout={timeout:.2f}ç§’ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰")
                return False
        except requests.exceptions.RequestException as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)
                output_func(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ï¼ˆå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: {filename}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                output_func(f"âŒ è¯·æ±‚å¼‚å¸¸: {filename} | {str(e)}ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰")
                return False
        except (ValueError, IOError) as e:
            # è¿™äº›é”™è¯¯é€šå¸¸ä¸éœ€è¦é‡è¯•ï¼ˆå¦‚æ ¼å¼é”™è¯¯ã€ä¿å­˜å¤±è´¥ç­‰ï¼‰
            output_func(f"âŒ ä¸‹è½½å¤±è´¥: {filename} | {str(e)}")
            return False
        except Exception as e:
            if attempt < max_retries:
                wait_time = retry_delay * (attempt + 1)
                output_func(f"âš ï¸ æœªçŸ¥é”™è¯¯ï¼ˆå°è¯• {attempt + 1}/{max_retries + 1}ï¼‰: {filename}ï¼Œ{wait_time:.1f}ç§’åé‡è¯•...")
                time.sleep(wait_time)
            else:
                output_func(f"âŒ æœªçŸ¥é”™è¯¯: {filename} | {str(e)}ï¼ˆå·²é‡è¯• {max_retries} æ¬¡ï¼‰")
                return False
    
    return False

def download_html(item, save_dir, timeout_min=8, timeout_max=12):
    """ä¸‹è½½ç½‘é¡µå…¬å‘Šï¼ˆHTMLæ ¼å¼ï¼‰"""
    if "adjunctUrl" in item and item["adjunctUrl"]:
        url = PDF_BASE + item["adjunctUrl"]
        if item["adjunctUrl"].lower().endswith(".pdf"):
            print(f"âš ï¸ è·³è¿‡PDFæ–‡ä»¶ï¼ˆåº”ä½¿ç”¨download_pdfï¼‰: {item.get('announcementTitle', 'Unknown')}")
            return False
    elif "announcementId" in item:
        url = f"https://www.cninfo.com.cn/new/disclosure/detail?plate=&orgId={item.get('orgId', '')}&stock={item.get('secCode', '')}&announcementId={item['announcementId']}&announcementTime={item.get('announcementTime', '')}"
    else:
        print(f"âš ï¸ æ— æ³•è·å–ç½‘é¡µå…¬å‘ŠURL: {item.get('announcementTitle', 'Unknown')}")
        return False

    sec_code = item.get('secCode', 'unknown')
    # åˆ›å»ºè‚¡ç¥¨ä»£ç å¯¹åº”çš„å­ç›®å½•
    stock_dir = os.path.join(save_dir, sec_code)
    if not os.path.exists(stock_dir):
        os.makedirs(stock_dir)
    
    announcement_time = get_announcement_date(item)
    filename_parts = [sec_code]
    if announcement_time:
        filename_parts.append(announcement_time)
    filename_parts.append(sanitize_filename(item.get('announcementTitle', 'Unknown')))
    filename = "_".join(filename_parts) + ".html"
    filepath = os.path.join(stock_dir, filename)

    try:
        timeout = get_random_timeout(timeout_min, timeout_max)
        html_resp = requests.get(url, timeout=timeout)
        
        if html_resp.status_code != 200:
            print(f"âš ï¸ HTMLä¸‹è½½å¤±è´¥: {filename} (çŠ¶æ€ç : {html_resp.status_code})")
            return False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯PDF
        content_type = html_resp.headers.get('content-type', '').lower()
        if 'application/pdf' in content_type or html_resp.content.startswith(b'%PDF'):
            print(f"âš ï¸ è·³è¿‡PDFæ–‡ä»¶ï¼ˆå†…å®¹æ£€æµ‹ï¼‰: {filename}")
            return False
        
        # æ£€æµ‹ç¼–ç 
        detected_encoding = html_resp.apparent_encoding
        if not detected_encoding or detected_encoding.lower() in ['iso-8859-1', 'ascii']:
            if 'charset=' in content_type:
                detected_encoding = content_type.split('charset=')[-1].strip().lower()
                detected_encoding = detected_encoding.strip('"\'')
            else:
                detected_encoding = 'utf-8'  # é»˜è®¤ä¸ºutf-8
        
        html_resp.encoding = detected_encoding
        html_text = html_resp.text
        
        # æ·»åŠ charsetå£°æ˜
        if '<head>' in html_text and 'charset=' not in html_text[:1000].lower():
            html_text = html_text.replace('<head>', f'<head>\n<meta charset="{detected_encoding}">', 1)
        
        # ä¿å­˜æ–‡ä»¶
        with open(filepath, "w", encoding=detected_encoding) as f:
            f.write(html_text)
        
        print(f"âœ… HTMLä¸‹è½½æˆåŠŸ: {filename} (ç¼–ç : {detected_encoding})")
        return True
        
    except Exception as e:
        print(f"âŒ HTMLä¸‹è½½é”™è¯¯: {filename} | {str(e)}")
        return False

def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="çˆ¬å–å·¨æ½®èµ„è®¯ç½‘å…¬å‘Šï¼ˆæ”¯æŒæŒ‰è‚¡ç¥¨ä»£ç ç­›é€‰ï¼‰",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
    ä½¿ç”¨ç¤ºä¾‹:
        # çˆ¬å–å•ä¸ªè‚¡ç¥¨ï¼ˆ000001 å¹³å®‰é“¶è¡Œï¼‰çš„10æ¡å…¬å‘Š
        python main_api_1118.py --stock-code 000001 --max-items-total 10
        
        # çˆ¬å–å¤šä¸ªè‚¡ç¥¨çš„å…¬å‘Šï¼ˆç”¨é€—å·åˆ†éš”ï¼‰
        python main_api_1118.py --stock-code 000001,600000 --max-items-total 20
        
        # ä»æ–‡ä»¶è¯»å–è‚¡ç¥¨ä»£ç åˆ—è¡¨å¹¶çˆ¬å–
        python main_api_1118.py --stock-file stockcodes/codes.txt --max-items-total 300 --save-dir data/announcements
        
        # åªç”Ÿæˆçˆ¬å–è®¡åˆ’æŠ¥å‘Šï¼Œä¸å®é™…æ‰§è¡Œ
        python main_api_1118.py --stock-file stockcodes/codes.txt --max-items-total 300 --plan-only
        
        # åªçˆ¬å–PDFæ ¼å¼ï¼Œä¸ä¸‹è½½HTML
        python main_api_1118.py --stock-code 000001 --max-items-total 5 --no-html
        
        # è‡ªå®šä¹‰é‡è¯•æ¬¡æ•°å’Œå»¶è¿Ÿï¼ˆé€‚ç”¨äºç½‘ç»œä¸ç¨³å®šç¯å¢ƒï¼‰
        python main_api_1118.py --stock-file stockcodes/codes.txt --max-items-total 300 --max-retries 5 --retry-delay 3.0
        
        # è‡ªå®šä¹‰è¶…æ—¶æ—¶é—´å’Œè¯·æ±‚å»¶è¿Ÿ
        python main_api_1118.py --stock-code 000001,600000 --max-items-total 50 --timeout-min 10 --timeout-max 15 --delay-min 2 --delay-max 5
        
        # çˆ¬å–æ‰€æœ‰è‚¡ç¥¨çš„å…¬å‘Šï¼ˆé»˜è®¤è¡Œä¸ºï¼Œä¸æŒ‡å®š--stock-codeå‚æ•°ï¼‰
        python main_api_1118.py --max-items-total 50
        """
    )
    
    parser.add_argument(
        "--stock-file",
        type=str,
        help="åŒ…å«è‚¡ç¥¨ä»£ç çš„æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæ”¯æŒçº¯æ•°å­—æˆ–å¸¦äº¤æ˜“æ‰€åç¼€ï¼‰"
    )

    parser.add_argument(
        "--stock-code",
        type=str,
        help="è‚¡ç¥¨ä»£ç ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªï¼Œç”¨é€—å·åˆ†éš”ï¼‰ï¼Œæ ¼å¼ï¼š600000ã€000001.SZã€600000.SH"
    )

    parser.add_argument(
        "--plan-only",
        action="store_true",
        help="åªç”Ÿæˆçˆ¬å–è®¡åˆ’æŠ¥å‘Šï¼Œä¸å®é™…è¯·æ±‚æˆ–ä¸‹è½½æ•°æ®"
    )

    parser.add_argument(
        "--max-items-total",
        type=int,
        default=100,
        help="æ‰€æœ‰è‚¡ç¥¨æœ€å¤šçˆ¬å–çš„å…¬å‘Šæ¡æ•° (é»˜è®¤: 100)"
    )
    
    parser.add_argument(
        "--save-dir",
        type=str,
        default="downloads",
        help="ä¿å­˜ç›®å½• (é»˜è®¤: downloads)"
    )
    
    parser.add_argument(
        "--timeout-min",
        type=float,
        default=8.0,
        help="æœ€å°è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ (é»˜è®¤: 8.0)"
    )
    
    parser.add_argument(
        "--timeout-max",
        type=float,
        default=12.0,
        help="æœ€å¤§è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ (é»˜è®¤: 12.0)"
    )
    
    parser.add_argument(
        "--delay-min",
        type=float,
        default=1.0,
        help="è¯·æ±‚ä¹‹é—´çš„æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰ (é»˜è®¤: 1.0)"
    )
    
    parser.add_argument(
        "--delay-max",
        type=float,
        default=3.0,
        help="è¯·æ±‚ä¹‹é—´çš„æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰ (é»˜è®¤: 3.0)"
    )
    
    parser.add_argument(
        "--download-delay-min",
        type=float,
        default=0.5,
        help="ä¸‹è½½ä¹‹é—´çš„æœ€å°å»¶è¿Ÿï¼ˆç§’ï¼‰ (é»˜è®¤: 0.5)"
    )
    
    parser.add_argument(
        "--download-delay-max",
        type=float,
        default=2.0,
        help="ä¸‹è½½ä¹‹é—´çš„æœ€å¤§å»¶è¿Ÿï¼ˆç§’ï¼‰ (é»˜è®¤: 2.0)"
    )
    
    parser.add_argument(
        "--page-size",
        type=int,
        default=30,
        help="æ¯é¡µè·å–çš„å…¬å‘Šæ•°é‡ (é»˜è®¤: 30)"
    )
    
    parser.add_argument(
        "--no-html",
        action="store_true",
        help="ä¸ä¸‹è½½HTMLæ ¼å¼çš„ç½‘é¡µå…¬å‘Šï¼Œåªä¸‹è½½PDF"
    )
    
    parser.add_argument(
        "--max-retries",
        type=int,
        default=3,
        help="è¯·æ±‚å¤±è´¥æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•° (é»˜è®¤: 3)"
    )
    
    parser.add_argument(
        "--retry-delay",
        type=float,
        default=2.0,
        help="é‡è¯•å»¶è¿ŸåŸºæ•°ï¼ˆç§’ï¼‰ï¼Œå®é™…å»¶è¿Ÿ = åŸºæ•° Ã— é‡è¯•æ¬¡æ•° (é»˜è®¤: 2.0)"
    )

    parser.add_argument(
        "--days",
        type=int,
        default=None,
        help="åªæŠ“å–è¿‘ N å¤©çš„å…¬å‘Šï¼ˆä»ä»Šå¤©å¾€å‰æ¨ç®—ï¼‰ï¼Œä¸ --max-items-total é…åˆä½¿ç”¨"
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    
    # å¤„ç†è‚¡ç¥¨ä»£ç 
    stock_codes = []
    if args.stock_code:
        # åˆ†å‰²å¤šä¸ªè‚¡ç¥¨ä»£ç 
        raw_codes = [code.strip() for code in args.stock_code.split(',') if code.strip()]
        try:
            # æ ‡å‡†åŒ–æ‰€æœ‰è‚¡ç¥¨ä»£ç 
            stock_codes = [normalize_stock_code(code) for code in raw_codes]
            print(f"ğŸ“‹ å·²è§£æè‚¡ç¥¨ä»£ç : {', '.join(stock_codes)}")
        except ValueError as e:
            print(f"âŒ è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯: {e}")
            exit(1)
    # å¦‚æœæä¾›äº†è‚¡ç¥¨æ–‡ä»¶ï¼Œåˆ™è¯»å–æ–‡ä»¶ä¸­çš„è‚¡ç¥¨ä»£ç 
    if args.stock_file:
        if not os.path.exists(args.stock_file):
            print(f"âŒ è‚¡ç¥¨æ–‡ä»¶ä¸å­˜åœ¨: {args.stock_file}")
            exit(1)

        with open(args.stock_file, "r", encoding="utf-8") as f:
            file_codes = [line.strip() for line in f if line.strip()]

        if not file_codes:
            print(f"âš ï¸ è‚¡ç¥¨æ–‡ä»¶ {args.stock_file} ä¸ºç©º")
        else:
            try:
                normalized_file_codes = [normalize_stock_code(code) for code in file_codes]
            except ValueError as e:
                print(f"âŒ è‚¡ç¥¨æ–‡ä»¶ä¸­çš„ä»£ç æ ¼å¼é”™è¯¯: {e}")
                exit(1)

            # åˆå¹¶å‘½ä»¤è¡Œå’Œæ–‡ä»¶ä¸­çš„ä»£ç ï¼Œå¹¶å»é‡
            merged_codes = stock_codes + normalized_file_codes
            # ä¿æŒåŸæœ‰é¡ºåºçš„åŒæ—¶å»é‡
            seen = set()
            stock_codes = []
            for code in merged_codes:
                if code not in seen:
                    seen.add(code)
                    stock_codes.append(code)

            print(f"ğŸ“‹ å·²ä»æ–‡ä»¶åŠ è½½è‚¡ç¥¨ä»£ç : {', '.join(normalized_file_codes)}")
    print(f"stock_codes: {stock_codes}")

    requested_codes = set(stock_codes)
    total_stocks = len(stock_codes) if stock_codes else "å…¨éƒ¨è‚¡ç¥¨"
    max_items = args.max_items_total
    page_size = args.page_size

    if stock_codes:
        estimated_total_items = total_stocks * max_items
    else:
        estimated_total_items = "æœªçŸ¥ï¼ˆéœ€æŒ‰æ¥å£è¿”å›è®¡ç®—ï¼‰"

    estimated_pages = (
        (estimated_total_items + page_size - 1) // page_size
        if isinstance(estimated_total_items, int)
        else "æœªçŸ¥"
    )

    print("\nğŸ“ çˆ¬å–è®¡åˆ’æŠ¥å‘Š")
    print(f"   è‚¡ç¥¨æ•°é‡: {total_stocks}")
    print(f"   æ€»ç›®æ ‡: {max_items} æ¡")
    print(f"   é¢„è®¡æ€»å…¬å‘Š: {estimated_total_items}")
    print(f"   é¢„è®¡APIé¡µæ•°: {estimated_pages} (page_size={page_size})")
    print(f"   è¯·æ±‚å»¶è¿Ÿ: {args.delay_min}-{args.delay_max} ç§’")
    if not args.no_html:
        print(f"   ä¸‹è½½å»¶è¿Ÿ: {args.download_delay_min}-{args.download_delay_max} ç§’ (PDF + HTML)")
    else:
        print(f"   ä¸‹è½½å»¶è¿Ÿ: {args.download_delay_min}-{args.download_delay_max} ç§’ (ä»… PDF)")

    if args.plan_only:
        print("\nğŸ“Œ plan-only æ¨¡å¼å¼€å¯ï¼Œä»…è¾“å‡ºæŠ¥å‘Šï¼Œä¸æ‰§è¡Œå®é™…è¯·æ±‚ã€‚")
        exit(0)

    # åˆ›å»ºä¿å­˜ç›®å½•
    if not os.path.exists(args.save_dir):
        os.makedirs(args.save_dir)
        print(f"ğŸ“ åˆ›å»ºä¿å­˜ç›®å½•: {args.save_dir}")
    
    # åŠ è½½å·²ä¸‹è½½çš„IDé›†åˆ
    downloaded_ids = load_downloaded_ids(args.save_dir)
    downloaded_ids_before = len(downloaded_ids)  # è®°å½•åˆå§‹æ•°é‡
    print(f"ğŸ“‹ å·²åŠ è½½ {downloaded_ids_before} ä¸ªå·²ä¸‹è½½å…¬å‘ŠID")
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print("\nğŸ“„ å¼€å§‹è¯·æ±‚å…¬å‘Šæ•°æ® ...")
    if stock_codes:
        print(f"   é…ç½®: è‚¡ç¥¨ä»£ç ={', '.join(stock_codes)}, æ€»ç›®æ ‡{args.max_items_total}æ¡")
    else:
        print(f"   é…ç½®: çˆ¬å–æ‰€æœ‰è‚¡ç¥¨ï¼Œæ€»ç›®æ ‡{args.max_items_total}æ¡")
    print(f"   ä¿å­˜åˆ°{args.save_dir}, timeout={args.timeout_min}-{args.timeout_max}ç§’")
    
    all_items = []
    
    # å¦‚æœæŒ‡å®šäº†è‚¡ç¥¨ä»£ç ï¼Œåˆ†æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹30åªï¼‰
    if stock_codes:
        batch_size = 30
        total_batches = (len(stock_codes) + batch_size - 1) // batch_size
        print(f"\nğŸ“¦ å°† {len(stock_codes)} åªè‚¡ç¥¨åˆ†æˆ {total_batches} æ‰¹å¤„ç†ï¼ˆæ¯æ‰¹ {batch_size} åªï¼‰")
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(stock_codes))
            batch_codes = stock_codes[start_idx:end_idx]
            
            print(f"\nğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {batch_idx + 1}/{total_batches} æ‰¹ï¼ˆè‚¡ç¥¨: {batch_codes[0]} ~ {batch_codes[-1]}ï¼‰...")
            
            page = 1
            batch_items = []
            
            # å¯¹å½“å‰æ‰¹æ¬¡å¾ªç¯ç¿»é¡µ
            while len(all_items) < args.max_items_total:
                print(f"   è¯·æ±‚ç¬¬ {page} é¡µ...")
                data = fetch_announcements(
                    stock_codes=batch_codes,
                    page_num=page,
                    page_size=args.page_size,
                    timeout_min=args.timeout_min,
                    timeout_max=args.timeout_max,
                    max_retries=args.max_retries,
                    retry_delay=args.retry_delay,
                    days=args.days 
                )
                
                if not data:
                    print(f"   âš ï¸ ç¬¬ {batch_idx + 1} æ‰¹ç¬¬ {page} é¡µæ²¡æœ‰æ›´å¤šæ•°æ®")
                    break
                
                # è¿‡æ»¤æ‰å·²ä¸‹è½½çš„å…¬å‘Šï¼ˆåŸºäºannouncementIdï¼‰
                new_items = []
                skipped_count = 0
                for item in data:
                    announcement_id = item.get('announcementId')
                    if announcement_id and announcement_id in downloaded_ids:
                        skipped_count += 1
                        continue
                    new_items.append(item)
                
                if skipped_count > 0:
                    print(f"   è·³è¿‡å·²ä¸‹è½½: {skipped_count} æ¡")
                
                # è®¡ç®—è¿˜èƒ½æ·»åŠ å¤šå°‘æ¡ï¼ˆåªè€ƒè™‘æœªä¸‹è½½çš„ï¼‰
                remaining = args.max_items_total - len(all_items)
                items_to_add = new_items[:remaining]
                
                batch_items.extend(items_to_add)
                all_items.extend(items_to_add)
                
                print(f"   ç¬¬ {page} é¡µè·å–åˆ° {len(items_to_add)} æ¡æ–°å…¬å‘Šï¼ˆæœ¬æ‰¹ç´¯è®¡: {len(batch_items)}ï¼Œæ€»è®¡: {len(all_items)}ï¼‰")
                
                if len(data) < args.page_size:
                    break
                
                if len(all_items) >= args.max_items_total:
                    print(f"   âœ… å·²è¾¾åˆ°æ€»ä¸Šé™ {args.max_items_total} æ¡ï¼Œåœæ­¢è¯·æ±‚")
                    break
                
                page += 1
                time.sleep(random.uniform(args.delay_min, args.delay_max))
            
            print(f"   ç¬¬ {batch_idx + 1} æ‰¹å®Œæˆï¼Œè·å– {len(batch_items)} æ¡æ–°å…¬å‘Š")
            
            # æ‰¹æ¬¡é—´å»¶è¿Ÿ
            if batch_idx < total_batches - 1:
                delay = random.uniform(args.delay_min, args.delay_max)
                print(f"   ç­‰å¾… {delay:.1f} ç§’åå¤„ç†ä¸‹ä¸€æ‰¹...")
                time.sleep(delay)
            
            # å¦‚æœå·²è¾¾åˆ°æ€»ä¸Šé™ï¼Œæå‰ç»“æŸ
            if len(all_items) >= args.max_items_total:
                print(f"\nâœ… å·²è¾¾åˆ°æ€»ä¸Šé™ï¼Œæå‰ç»“æŸæ‰¹æ¬¡å¤„ç†")
                break
    else:
        # æœªæŒ‡å®šè‚¡ç¥¨ä»£ç ï¼Œä½¿ç”¨åŸæ¥çš„é€»è¾‘ï¼ˆå…¨å¸‚åœºï¼‰
        page = 1
        while len(all_items) < args.max_items_total:
            print(f"\nğŸ“„ æ­£åœ¨è¯·æ±‚ç¬¬ {page} é¡µå…¬å‘Šæ•°æ® ...")
            data = fetch_announcements(
                stock_codes=None,
                page_num=page,
                page_size=args.page_size,
                timeout_min=args.timeout_min,
                timeout_max=args.timeout_max,
                max_retries=args.max_retries,
                retry_delay=args.retry_delay,
                days=args.days
            )

            if not data:
                print("âš ï¸ æ²¡æœ‰æ›´å¤šæ•°æ®äº†")
                break

            # è¿‡æ»¤æ‰å·²ä¸‹è½½çš„å…¬å‘Šï¼ˆåŸºäºannouncementIdï¼‰
            new_items = []
            skipped_count = 0
            for item in data:
                announcement_id = item.get('announcementId')
                if announcement_id and announcement_id in downloaded_ids:
                    skipped_count += 1
                    continue
                new_items.append(item)
            
            if skipped_count > 0:
                print(f"   è·³è¿‡å·²ä¸‹è½½: {skipped_count} æ¡")

            # è®¡ç®—è¿˜èƒ½æ·»åŠ å¤šå°‘æ¡ï¼ˆåªè€ƒè™‘æœªä¸‹è½½çš„ï¼‰
            remaining = args.max_items_total - len(all_items)
            items_to_add = new_items[:remaining]
            all_items.extend(items_to_add)
            
            print(f"   ç¬¬ {page} é¡µè·å–åˆ° {len(items_to_add)} æ¡æ–°å…¬å‘Šï¼ˆæ€»è®¡: {len(all_items)}ï¼‰")
            
            if len(data) < args.page_size:
                break
            
            if len(all_items) >= args.max_items_total:
                print(f"   âœ… å·²è¾¾åˆ°æ€»ä¸Šé™ {args.max_items_total} æ¡ï¼Œåœæ­¢è¯·æ±‚")
                break
            
            page += 1
            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(args.delay_min, args.delay_max))

    print(f"\nâœ… å…±è·å– {len(all_items)} æ¡å…¬å‘Š")
    
    # æŒ‰è‚¡ç¥¨ä»£ç åˆ†ç»„ï¼ˆä¾¿äºç»Ÿè®¡ï¼‰
    stock_groups = {}
    for item in all_items:
        sec_code = item.get('secCode', 'unknown')
        if sec_code not in stock_groups:
            stock_groups[sec_code] = []
        stock_groups[sec_code].append(item)
    
    print(f"ğŸ“Š å„è‚¡ç¥¨å…¬å‘Šæ•°é‡:")
    for sec_code, items in stock_groups.items():
        print(f"   {sec_code}: {len(items)} æ¡")

    missing_codes = sorted([
        code for code in requested_codes
        if code not in stock_groups or len(stock_groups[code]) == 0
    ])
    if missing_codes:
        print("\nâš ï¸ ä»¥ä¸‹è‚¡ç¥¨æœªè·å–åˆ°ä»»ä½•å…¬å‘Šï¼š")
        for code in missing_codes:
            print(f"   {code}")
    else:
        print("\nâœ… æ‰€æœ‰æŒ‡å®šè‚¡ç¥¨å‡è·å–åˆ°è‡³å°‘ä¸€æ¡å…¬å‘Š")
    # åˆ†åˆ«å¤„ç†PDFå’Œç½‘é¡µå…¬å‘Š
    pdf_items = [i for i in all_items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")]
    html_items = [] if args.no_html else [i for i in all_items if i not in pdf_items]

    print(f"\nå‡†å¤‡ä¸‹è½½ {len(pdf_items)} ä»½PDFå…¬å‘Š", end="")
    if not args.no_html:
        print(f" å’Œ {len(html_items)} ä»½ç½‘é¡µå…¬å‘Š", end="")
    print("...")
    
    success_pdf = 0
    success_html = 0
    
    # ä¸‹è½½PDFå…¬å‘Š
    print(f"\nå¼€å§‹ä¸‹è½½ {len(pdf_items)} ä»½PDFå…¬å‘Š...")
    for item in tqdm(pdf_items, desc="ä¸‹è½½PDF", unit="ä»½", ncols=100):
        if download_pdf(item, args.save_dir, downloaded_ids, args.timeout_min, args.timeout_max, output_func=tqdm.write, max_retries=args.max_retries, retry_delay=args.retry_delay):
            success_pdf += 1
        time.sleep(random.uniform(args.download_delay_min, args.download_delay_max))
    print()
    
    # ä¸‹è½½ç½‘é¡µå…¬å‘Š
    if not args.no_html:
        for item in html_items:
            if download_html(item, args.save_dir, args.timeout_min, args.timeout_max):
                success_html += 1
            time.sleep(random.uniform(args.download_delay_min, args.download_delay_max))
    
    print(f"\nğŸ¯ ä¸‹è½½å®Œæˆï¼")
    print(f"   PDF: {success_pdf}/{len(pdf_items)} ä»½")
    if not args.no_html:
        print(f"   HTML: {success_html}/{len(html_items)} ä»½")
    print(f"   æ€»è®¡: {success_pdf + success_html}/{len(all_items)} ä»½")

    # ä¿å­˜å·²ä¸‹è½½IDé›†åˆ
    save_downloaded_ids(args.save_dir, downloaded_ids)
    print(f"ğŸ’¾ å·²ä¿å­˜ {len(downloaded_ids)} ä¸ªå·²ä¸‹è½½å…¬å‘ŠIDåˆ° {os.path.join(args.save_dir, '.downloaded_ids.json')}")
    
    # ç”Ÿæˆä¸‹è½½æŠ¥å‘Š
    generate_download_report(
        save_dir=args.save_dir,
        stock_codes=stock_codes,
        all_items=all_items,
        stock_groups=stock_groups,
        success_pdf=success_pdf,
        success_html=success_html,
        pdf_items=pdf_items,
        html_items=html_items,
        requested_codes=requested_codes,
        missing_codes=missing_codes,
        downloaded_ids_before=downloaded_ids_before,
        downloaded_ids_after=downloaded_ids,
        args=args
    )
    
    # æ‰“å°æ¯ä¸ªè‚¡ç¥¨çš„ä¸‹è½½ç»Ÿè®¡
    print(f"\nğŸ“ˆ å„è‚¡ç¥¨ä¸‹è½½ç»Ÿè®¡:")
    for sec_code, items in stock_groups.items():
        sec_pdf = len([i for i in items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")])
        sec_html = len(items) - sec_pdf if not args.no_html else 0
        sec_success_pdf = len([i for i in items if "adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf") and os.path.exists(os.path.join(args.save_dir, sec_code, f"{sec_code}_{get_announcement_date(i)}_{sanitize_filename(i.get('announcementTitle', 'Unknown'))}.pdf"))])
        sec_success_html = 0
        if not args.no_html:
            sec_success_html = len([i for i in items if not ("adjunctUrl" in i and i["adjunctUrl"].lower().endswith(".pdf")) and os.path.exists(os.path.join(args.save_dir, sec_code, f"{sec_code}_{get_announcement_date(i)}_{sanitize_filename(i.get('announcementTitle', 'Unknown'))}.html"))])
        print(f"   {sec_code}: PDF {sec_success_pdf}/{sec_pdf} ä»½, HTML {sec_success_html}/{sec_html} ä»½")